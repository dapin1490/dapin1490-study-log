---
title: "정보 검색"
author: dapin1490
date: 2022-10-16T20:39:00+09:00
categories: [지식, IT]
tags: [지식, IT, 정보 검색, 필기]
render_with_liquid: false
---

<style>
	.x-understand { color: #ccb833; }
	.understand { color: #1380da; }
	.tab { white-space: pre; }
    .underline { text-decoration: underline; }
    figure { text-align: center; }
</style>

## 2주

이번 학기 공부 대상 : 텍스트 검색

유닉스 명령어 `grep` : 원하는 것을 찾을 수 있긴 한데 정보 검색용으로 있는 명령어는 아님  
- 특정 단어가 존재하거나, 존재하지 않는 파일 검색 가능
- 대규모 파일 처리 느림
- 없는 단어 찾기 어려움 : 전수조사 필요
- "~와 가까운 ~" 같은 상세조건 불가
- 적절한 결과순 랭킹 불가
- `*` : 모든 파일 중에서 찾기
- `|` : 이 기호 앞에 있는 명령을 먼저 수행하고, 그 결과에 대해 다음 명령을 수행
- `-v` : NOT

term-doc matrix : term과 doc의 관계를 나타낸 행렬
- 행에는 모든 term 나열, 열에는 모든 doc 나열, 각 term과 doc에 대해 등장 해당 doc에 term 등장 여부 표시
- 문서가 많을수록 행렬이 커짐
- 비트 단위 연산으로 간단히 결과 도출 가능

정보 검색의 기본적인 전제 : 문서의 집합은 고정되어 있다(비현실적이지만 설명 편의상 전제함)  

**정밀도와 재현율 : 통합본 13페이지, 2주차 13페이지**  
정밀도 : 검색 결과 중 제대로 검색한 것  
재현율 : 원래 원했던 결과 중 찾아낸 것  
F-measure : 가중치를 준 정밀도와 재현율의 조화평균. 보통 가중치는 0.5로 주고 이를 정리하면 2PR/(P+R)이 됨.  

inverted index : 각 단어마다 그것이 등장하는 문서만 표시한 인덱스. 단어를 헤드노드로 하는 연결 리스트로 만든다. 정렬된 게 쓰기 좋다.
- 토큰화 과정 : 모든 문서를 검사해 (단어, 등장 문서) 투플을 만들고, 단어를 기준으로 정렬하고, 중복된 단어에 대해 투플을 통합한다. 문서당 빈도를 표시할 수도 있다.  
    결과는 사전과 포스팅 리스트로 나누어진다.

사전은 상대적으로 크기가 작으므로 보통 메인 메모리에 저장하고 포스팅은 디스크에 저장한다. 포스팅 리스트는 연결 리스트로 만든다.
비교해야 할 두 단어의 포스팅 리스트 길이가 각각 x, y라고 할 때, AND 연산의 시간 복잡도는 O(x + y)이다. 이때 리스트는 정렬되어 있어야 한다.
여러 단어에 대해 논리 연산을 해야 할 경우 연산 순서를 바꾸면 연산 시간을 줄일 수도 있다. 포스팅 리스트(doc-freq) 길이가 짧은 것부터 먼저 연산하면 된다. OR 연산의 결과의 길이는 최대 두 포스팅 리스트의 합과 으므로 AND 연산과 섞여있다면 최댓값으로 가정하고 순서를 정한다.

위의 inverted index가 불가능한 기능 : 구 검색, "가까운" 단어 검색, 검색할 정보의 zone 지정하기

"가까운" 단어 검색 : 포스팅 리스트에 있는 각 문서별로 등장 페이지 리스트를 따로 붙이면 계산 가능
검색 결과 정렬에는 term-freq를 활용할 수 있으나 신뢰도는 보장할 수 없음 -> boolean 모델에서 원칙적으로 결과 랭킹은 불가하지만 가능은 함

텍스트 검색은 범위(부등호) 검색 불가능

군집과 분류 : 통합본 46페이지, 2주차 46페이지
군집 : 선묶음 후기준, 총 묶음 개수를 미리 알 수 없음
분류 : 선기준 후묶음, 묶음 개수가 한정됨


## 3주

문서 토큰화 이전에 할 일 : 문서 종류 파악, 언어 파악(코드 변환이 필요할 수 있음) -> 기계가 하거나, 사람이 하거나
한 문서가 여러 언어로 쓰인 경우, 한 문서가 여러 파일/언어로 구성된 경우 있을 수 있음

**토큰화**
토큰화 : 문서를 단어 단위로 나누는 것, 정규화 이후 진행함, 불용어 사용. 기본적으로는 공백 단위로 나누고 문장부호 삭제하는데, 고유명사에 문장부호가 들어가는 등의 경우는 어떤 방식을 쓰든 문서와 질의어를 똑같이 전처리하면 된다.

어려운 점 : 하이픈의 용도가 다양해서 처리 방법을 하나로 정할 수 없음, 하이픈이 들어간 구는 더 골때림
문제점 : 날짜, IP주소, 이메일 등 기호/공백이 있지만 분리하면 안되는 토큰 -> 무시하기엔 너무 유용하고 토큰 넣기엔 너무 많은데 -> 사전을 늘리기로 결정

토큰화 언어 식별 : 숫자로 된 코드만 보고 한글인지 영어인지 맞혀야 함. 영어에 없는 조합이면 한국어고, 한국어에 없는 조합이면 영어라고 판단하는 방식으로 식별함.

**불용어**
불용어 : 빈번하게 나오지만 별로 중요하지 않은 말, 기능어(반대 : 내용어).
전체 텍스트의 약 30% 차지, 사전 크기는 별로 줄지 않지만 포스팅 리스트는 많이 줄일 수 있다.
문장이 너무 짧은 경우 불용어가 문장의 대부분을 차지해 문장의 내용이 상당히 소실되는 문제가 있어 불용어가 줄어들고 있으나, 요즘은 압축 기술이 좋아 포스팅 리스트 저장 비용이 감소했고, 질의어를 잘 최적화하면 된다(기능어에 가중치 적게 주기).

**정규화**
정규화 : 다르게 표기하지만 실상은 다 같은 단어를 하나의 단어로 통합하는 것. 기호를 없애는 것도 방법.
Thesaurus : 유의어/반의어 사전. 동의어나 동음이의어를 다룰 수 있으며 이 사전이 없으면 수작업을 해야 함.
정규화를 할 경우 질의어도 똑같이 정규화해서 검색해야 한다.
- 단어를 하나의 클래스로 통합해 인덱스를 합치는 방법이 있고
- 하나의 클래스에 속하는 단어마다 모두 통합된 똑같은 인덱스를 주는 방법이 있고 -> 메모리를 손해보지만 질의어를 정규화할 필요가 없어 검색이 빨라짐
- 각 단어별 인덱스는 따로 만들고 질의어를 확장하는 방법 -> 오래걸림



통합본 81페이지부터 이어서
