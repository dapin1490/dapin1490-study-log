---
title: "정보 검색"
author: dapin1490
date: 2022-10-16T20:39:00+09:00
categories: [지식, IT]
tags: [지식, IT, 정보 검색, 필기]
render_with_liquid: false
---

<style>
	.x-understand { color: #ccb833; }
	.understand { color: #1380da; }
	.tab { white-space: pre; }
    .underline { text-decoration: underline; }
    figure { text-align: center; }
</style>

## 2주

이번 학기 공부 대상 : 텍스트 검색

유닉스 명령어 `grep` : 원하는 것을 찾을 수 있긴 한데 정보 검색용으로 있는 명령어는 아님
- 특정 단어가 존재하거나, 존재하지 않는 파일 검색 가능
- 대규모 파일 처리 느림
- 없는 단어 찾기 어려움 : 전수조사 필요
- "~와 가까운 ~" 같은 상세조건 불가
- 적절한 결과순 랭킹 불가
- `*` : 모든 파일 중에서 찾기
- `|` : 이 기호 앞에 있는 명령을 먼저 수행하고, 그 결과에 대해 다음 명령을 수행
- `-v` : NOT

term-doc matrix : term과 doc의 관계를 나타낸 행렬
- 행에는 모든 term 나열, 열에는 모든 doc 나열, 각 term과 doc에 대해 등장 해당 doc에 term 등장 여부 표시
- 문서가 많을수록 행렬이 커짐
- 비트 단위 연산으로 간단히 결과 도출 가능

정보 검색의 기본적인 전제 : 문서의 집합은 고정되어 있다(비현실적이지만 설명 편의상 전제함)

**정밀도와 재현율 : 통합본 13페이지, 2주차 13페이지**
정밀도 : 검색 결과 중 제대로 검색한 것
재현율 : 원래 원했던 결과 중 찾아낸 것
F-measure : 가중치를 준 정밀도와 재현율의 조화평균. 보통 가중치는 0.5로 주고 이를 정리하면 2PR/(P+R)이 됨.

inverted index : 각 단어마다 그것이 등장하는 문서만 표시한 인덱스. 단어를 헤드노드로 하는 연결 리스트로 만든다. 정렬된 게 쓰기 좋다.
- 토큰화 과정 : 모든 문서를 검사해 (단어, 등장 문서) 투플을 만들고, 단어를 기준으로 정렬하고, 중복된 단어에 대해 투플을 통합한다. 문서당 빈도를 표시할 수도 있다.
    결과는 사전과 포스팅 리스트로 나누어진다.

사전은 상대적으로 크기가 작으므로 보통 메인 메모리에 저장하고 포스팅은 디스크에 저장한다. 포스팅 리스트는 연결 리스트로 만든다.
비교해야 할 두 단어의 포스팅 리스트 길이가 각각 x, y라고 할 때, AND 연산의 시간 복잡도는 O(x + y)이다. 이때 리스트는 정렬되어 있어야 한다.
여러 단어에 대해 논리 연산을 해야 할 경우 연산 순서를 바꾸면 연산 시간을 줄일 수도 있다. 포스팅 리스트(doc-freq) 길이가 짧은 것부터 먼저 연산하면 된다. OR 연산의 결과의 길이는 최대 두 포스팅 리스트의 합과 으므로 AND 연산과 섞여있다면 최댓값으로 가정하고 순서를 정한다.

위의 inverted index가 불가능한 기능 : 구 검색, "가까운" 단어 검색, 검색할 정보의 zone 지정하기

"가까운" 단어 검색 : 포스팅 리스트에 있는 각 문서별로 등장 페이지 리스트를 따로 붙이면 계산 가능
검색 결과 정렬에는 term-freq를 활용할 수 있으나 신뢰도는 보장할 수 없음 -> boolean 모델에서 원칙적으로 결과 랭킹은 불가하지만 가능은 함

텍스트 검색은 범위(부등호) 검색 불가능

군집과 분류 : 통합본 46페이지, 2주차 46페이지
군집 : 선묶음 후기준, 총 묶음 개수를 미리 알 수 없음
분류 : 선기준 후묶음, 묶음 개수가 한정됨


## 3주

문서 토큰화 이전에 할 일 : 문서 종류 파악, 언어 파악(코드 변환이 필요할 수 있음) -> 기계가 하거나, 사람이 하거나
한 문서가 여러 언어로 쓰인 경우, 한 문서가 여러 파일/언어로 구성된 경우 있을 수 있음

**토큰화**
토큰화 : 문서를 단어 단위로 나누는 것, 정규화 이후 진행함, 불용어 사용. 기본적으로는 공백 단위로 나누고 문장부호 삭제하는데, 고유명사에 문장부호가 들어가는 등의 경우는 어떤 방식을 쓰든 문서와 질의어를 똑같이 전처리하면 된다.

어려운 점 : 하이픈의 용도가 다양해서 처리 방법을 하나로 정할 수 없음, 하이픈이 들어간 구는 더 골때림
문제점 : 날짜, IP주소, 이메일 등 기호/공백이 있지만 분리하면 안되는 토큰 -> 무시하기엔 너무 유용하고 토큰 넣기엔 너무 많은데 -> 사전을 늘리기로 결정

토큰화 언어 식별 : 숫자로 된 코드만 보고 한글인지 영어인지 맞혀야 함. 영어에 없는 조합이면 한국어고, 한국어에 없는 조합이면 영어라고 판단하는 방식으로 식별함.

**불용어**
불용어 : 빈번하게 나오지만 별로 중요하지 않은 말, 기능어(반대 : 내용어).
전체 텍스트의 약 30% 차지, 사전 크기는 별로 줄지 않지만 포스팅 리스트는 많이 줄일 수 있다.
문장이 너무 짧은 경우 불용어가 문장의 대부분을 차지해 문장의 내용이 상당히 소실되는 문제가 있어 불용어가 줄어들고 있으나, 요즘은 압축 기술이 좋아 포스팅 리스트 저장 비용이 감소했고, 질의어를 잘 최적화하면 된다(기능어에 가중치 적게 주기).

**정규화**
정규화 : 다르게 표기하지만 실상은 다 같은 단어를 하나의 단어로 통합하는 것. 기호를 없애는 것도 방법.
Thesaurus : 유의어/반의어 사전. 동의어나 동음이의어를 다룰 수 있으며 이 사전이 없으면 수작업을 해야 함.
정규화를 할 경우 질의어도 똑같이 정규화해서 검색해야 한다.
- 단어를 하나의 클래스로 통합해 인덱스를 합치는 방법이 있고
- 하나의 클래스에 속하는 단어마다 모두 통합된 똑같은 인덱스를 주는 방법이 있고 -> 메모리를 손해보지만 질의어를 정규화할 필요가 없어 검색이 빨라짐
- 각 단어별 인덱스는 따로 만들고 질의어를 확장하는 방법 -> 오래걸림



**Stemming**
재현율(recall) 높이기 위해 단어에서 접사 떼고 어근만 남기기
정확할 필요 없고 적당히 자르면 됨. `sess` -> `ss` 등 규칙 있음.
질의어도 똑같이 처리해야 함

**Lemmatization**
형태소 분석. 엄격하게 실제 단어만 남기고 어근/조사/접사 다 떼기

* 스테밍과 레마티제이션 비교 : 후자가 더 좋아보이지만 실상 별 차이 없다고 함


**스킵 포인터**
인덱싱할 때 만듦. 검색하기 위한 비교 횟수 감소 기능.
스킵 포인트가 많으면 스킵 기회도 많지만 스킵 거리가 짧고 메모리 더 필요
스킵 포인트가 적으면 스킵 기회는 적지만 스킵 거리가 긺.


**구 검색**
1. biword index : 모든 조합의 두 단어 구(인접한 것만) 인덱스 만들기
    사전이 매우 커지지만 두 단어 구 즉시 검색 가능
    긴 구 질의 - 여러 개의 두 단어 구로 나눠 검색, 검색이 되긴 되지만 꼭 모든 단어가 질의처럼 붙어서 나온다는 보장이 없어 위양성(false positive) 존재
    표준적인 해결책이 아님
2. positional index 등장 위치 인덱스 : 단어에 달린 포스팅 리스트의 각 포스트마다 해당 단어가 어디서 등장하는지 인덱스 기록. 사전은 그대로이고 인덱스 사이즈 커짐.
    두 단어 구 검색 - 두 단어 동시 등장 문서 찾기, 동시 등장 문서 중 인접한 위치 찾기
    구가 얼마나 길든 검색 가능, 단어 간 거리 지정 검색 가능 <- biword는 불가능
3. 혼합 방식 : 자주 검색되는 특정 구에 대해 biword 인덱스 만들기
    사람들이 검색한 기록인 로그 데이터(자산이 되기 때문에 함부로 버리면 안됨)를 이용해 자주 검색되는 구를 알아내 biword 인덱스 생성


## 4주

사전을 저장하는 자료구조 - 주로 두 가지 방법 : 해시테이블과 트리

**해시 테이블**
해시 함수에 문자열을 입력으로 주면 함수 계산을 해서 나오는 값을 인덱스로 삼아 데이터를 저장, 해시 함수를 잘 만들어야 효율적으로 저장할 수 있다.
탐색 속도가 아주 빠르지만 해시 함수에 따라 저장 공간이 중복되는 단어(충돌)가 생길 수 있다 -> 충돌한 자리에 연결 리스트 생성
장점 : 서치가 O(1)
단점 : 비슷한 단어 찾기 어려움(해시 값이 전혀 다름), 와일드카드 불가, 사전이 점점 커지면서 해시테이블의 크기를 바꾸면 해시함수도 바뀌고 사전 전체를 다시 해싱해야 한다

**트리 - 이진 트리**
장점 : 비슷한 단어 찾을 수 있음, 와일드카드 가능
단점 : 서치 느림 O(logn), 효율적이려면 균형 트리 만들어야 함(루트를 잘 골라야 하고 정렬된 데이터 넣으면 skewed), 트리를 수정하다 보면 균형 깨질 수 있음(-> 재정렬 필요) -> 재정렬 피하기 위해 B트리 사용

**트리 - B트리**
자식 노드의 수가 a~b개 사이를 유지하는 트리


**와일드카드 쿼리**
lexicon : 사전
B트리를 쓰면 와일드카드 질의를 다루기 쉽다
결과는 검색된 단어들을 전부 검색(문서 검색이 목적이니까)

- 와일드카드가 앞에 나오면? `*mon`
    거꾸로 된 B트리(단어를 거꾸로 쓴 것)가 하나 더 필요하다
    역B트리로 `nom*` 검색
- 중간에 와일트카드가 들어간 경우 `pro*cent`
    반으로 나눠서 따로 검색하고 결과 합치기, 그 결과로 나온 단어로 다시 문서 검색
    문제 : 검색 결과 합치는 연산이 너무 오래 걸림 -> permuterm index로 시간이 덜 걸리는 대신 메모리를 더 씀

**permuterm index**
단어에 $를 붙이고 회전시킴, 이 회전을 permute라고 함.
변형된 단어를 모두 사전에 저장하고 원본 단어를 가리키는 포인터를 둠.
기본 와일드카드 검색은 질의어 끝에 $를 붙이고 단어를 회전시켜 \*이 마지막에 나오게 만듦.
와일드카드가 여러 개 있으면 $를 붙인 상태에서 우선 그 와일드카드 중 하나가 단어 맨 뒤로 가게 만들어 앞에서부터 와일드카드 단위로 잘라서 검색
단점 : 사전이 많이 커진다. 영어로 실험 결과 사전이 10배 정도 커지지만 AND 연산보다는 감수할만 하다고 함.

**k-gram**
보통 n그램이라고 많이 씀. k개의 연속된 시퀀스(**글자**, 단어 등)를 말함. $로 단어의 시작과 끝을 표시.
1-gram : unigram, 2-gram : bigram, 3-gram : trigram, n-gram : ngram. 주로 2그램, 3그램 씀.

**k-gram index**
모든 k그램을 사전에 저장
각 k그램은 해당 k그램을 포함하는 단어를 전부 가리킴, 사전순 정렬
3그램이 2그램보다 단어 종류가 많지만(메모리 손해) 효율도 더 좋음
permuterm 인덱스보다 메모리는 효율적(중복이 많아서 사전이 덜 큼), 속도는 비교하기 어려움
문제 : 와일드카드 검색어에 따라 올바르지 않은 결과가 섞일 수 있다 -> 검색 후 필터링하면 됨

**철자 고쳐주기**
- 단어 고쳐주기
1. 단어만 고려하기(문맥 X)
2. 문맥 고려해서 고쳐주기(시간이 좀 더 오래 걸림)

- 문서 고쳐주기
이미지 -> 텍스트 변환한 문서의 오타 고치기. 문자 인식 오타와 타자 오타는 양상이 다름.

멋대로 고쳐서 검색하지 말고 사용자에게 제안하기

사용자의 검색어와 비슷한 단어 찾기 : 만약 고친 단어 후보 중 동점이 나와서 결정이 안 되면 단어 사용 횟수/검색 빈도 고려. 어떻게 가장 비슷한 단어를 정하나? -> 편집 거리, k그램 오버랩

전제 : 정확한 단어의 사전이 있다 -> 사전에서 질의어와 비슷하게 생긴 단어를 찾음

**편집 거리**
단어를 고치려면 몇 번이나 편집해야 하는지
기본적으로 삽입과 삭제만을 고려하고, '대체'도 포함할지는 상황에 따라 다름.
가로로는 올바른 단어를 쓰고, 세로로는 잘못 쓴 단어를 써서 각 글자를 연결해 격자를 만든다. 격자의 왼쪽 위에서부터 오른쪽 아래로 가는 최단경로가 편집 거리. 아래로 가는 방향은 글자 삭제를 의미하고 오른쪽으로 가는 방향은 글자 삽입을 의미함. 서로 겹치는 글자가 만나는 격자에는 대각선을 그어 카운트하지 않고 넘어가게 한다.

편집 거리가 특정 값보다 작은 것만 대체 단어로 제안하도록 하면 제안할 대상을 줄일 수 있다.

**k그램 오버랩**
질의어를 이용해 k그램 검색을 하여 질의어가 갖는 k그램을 마찬가지로 갖는 단어들을 찾되, 질의어는 틀린 단어이기 때문에 그대로 검색하면 안된다. threshold를 정해서 이 값을 넘기는 것을 후보로 포함한다.
k그램 오버랩으로 골라낸 단어만 가지고 편집 거리 계산
문제 : 단어가 너무 길어서 k그램이 많이 겹쳤을 뿐인 사례. 정규화는 어떻게 할 수 있는지.

k그램 오버랩 : 두 집합이 얼마나 겹치는지 계산. 교집합 크기 / 합집합 크기. 각 집합의 요소는 해당 단어의 k그램.

**문맥 고려 단어 고치기**
모든 단어가 틀렸다고 전제하고 교정 후보 찾아 조합하기 -> 그 조합 중 가장 많이 나온 것으로 제안(말뭉치에 단어 3그램 미리 준비)
이 방법은 정말 오래걸리니까 수상할 정도로 검색 결과가 적을 때만 사용


## 5주
인덱스 만들기

**하드웨어 기초**
메인 메모리 접근 시간 < 하드 접근 시간
디스크 탐색 시간 : 헤드가 원하는 트랙과 섹터로 이동하는 시간
IO는 블록 단위로. 컴퓨터마다 다르지만 보통 한 블록은 8~64kb

압축하지 않은 데이터 읽는 시간 > 압축한 데이터 읽고 압축 푸는 시간 -> 요즘은 이게 더 빠름

Fault tolerance : 기계적인 결함이 발생해도 시스템을 계속 사용할 수 있는 시스템(예: CPU가 여러 개 있어서 하나가 죽어도 계속 작동함)
비싼 컴퓨터 하나 쓰는 것보다 싼 컴퓨터 여러 대 묶어서 쓰는 게 저렴하고 낫다.


**로이터 RCV1**
로이터의 1년치 뉴스기사로 만든 말뭉치, 다양한 토픽 존재함.
스케일에 상관 없이 먹히는 인덱스 구축 알고리즘을 적용하는 예시로 사용.

토큰 당 평균 바이트 수는 4.5, term당 평균 바이트 수는 7.5인데 이 차이는 불용어 유무 때문에 생김. 보통 불용어는 짧고 많기 때문에 이것이 빠진 term의 평균 길이가 길어짐.


통합본 173페이지부터 이어서